{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from cv2 import *\n",
    "import PIL\n",
    "from PIL import Image,ImageTk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "window = tk.Tk()\n",
    "window.title(\"License Plate Recognition\")\n",
    "canvas = tk.Canvas(window)\n",
    "canvas.pack(side=RIGHT)\n",
    "btn_blur=tk.Button(window, text=\"Load Image\",width=50,height=2,command=selectfile)\n",
    "btn_blur.pack(anchor=tk.CENTER, expand=True)\n",
    "result=tk.Label(window,text=\"Result Here\")\n",
    "result.config(font=(\"Courier\", 44))\n",
    "objects=tk.Label(window,text=\"\")\n",
    "objects.config(font=(\"Courier\",30))\n",
    "extract=tk.Label(window,text=\"\")\n",
    "objects.pack()\n",
    "extract.pack()\n",
    "result.pack()\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ObjectDetection(object):\n",
    "    \"\"\"Class for Custom Vision's exported object detection model\n",
    "    \"\"\"\n",
    "\n",
    "    ANCHORS = np.array([[0.573, 0.677], [1.87, 2.06], [3.34, 5.47], [7.88, 3.53], [9.77, 9.17]])\n",
    "    IOU_THRESHOLD = 0.45\n",
    "\n",
    "    def __init__(self, labels, prob_threshold = 0.10, max_detections = 20):\n",
    "        \"\"\"Initialize the class\n",
    "\n",
    "        Args:\n",
    "            labels ([str]): list of labels for the exported model.\n",
    "            prob_threshold (float): threshold for class probability.\n",
    "            max_detections (int): the max number of output results.\n",
    "        \"\"\" \n",
    "\n",
    "        assert len(labels) >= 1, \"At least 1 label is required\"\n",
    "\n",
    "        self.labels = labels\n",
    "        self.prob_threshold = prob_threshold\n",
    "        self.max_detections = max_detections\n",
    "\n",
    "    def _logistic(self, x):\n",
    "        return np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "    def _non_maximum_suppression(self, boxes, class_probs, max_detections):\n",
    "        \"\"\"Remove overlapping bouding boxes\n",
    "        \"\"\"\n",
    "        assert len(boxes) == len(class_probs)\n",
    "        if len(boxes)<max_detections:\n",
    "            max_detections=len(boxes)\n",
    "        max_probs = np.amax(class_probs, axis=1)\n",
    "        max_classes = np.argmax(class_probs, axis=1)\n",
    "\n",
    "        areas = boxes[:,2] * boxes[:,3]\n",
    "\n",
    "        selected_boxes = []\n",
    "        selected_classes = []\n",
    "        selected_probs = []\n",
    "        while len(selected_boxes) < max_detections:\n",
    "            # Select the prediction with the highest probability.\n",
    "            i = np.argmax(max_probs)\n",
    "            if max_probs[i] < self.prob_threshold:\n",
    "                break\n",
    "\n",
    "            # Save the selected prediction\n",
    "            selected_boxes.append(boxes[i])\n",
    "            selected_classes.append(max_classes[i])\n",
    "            selected_probs.append(max_probs[i])\n",
    "\n",
    "            box = boxes[i]\n",
    "            other_indices = np.concatenate((np.arange(i), np.arange(i+1,len(boxes))))\n",
    "            other_boxes = boxes[other_indices]\n",
    "\n",
    "            # Get overlap between the 'box' and 'other_boxes'\n",
    "            x1 = np.maximum(box[0], other_boxes[:,0])\n",
    "            y1 = np.maximum(box[1], other_boxes[:,1])\n",
    "            x2 = np.minimum(box[0]+box[2], other_boxes[:,0]+other_boxes[:,2])\n",
    "            y2 = np.minimum(box[1]+box[3], other_boxes[:,1]+other_boxes[:,3])\n",
    "            w = np.maximum(0, x2 - x1)\n",
    "            h = np.maximum(0, y2 - y1)\n",
    "\n",
    "            # Calculate Intersection Over Union (IOU)\n",
    "            overlap_area = w * h\n",
    "            iou = overlap_area / (areas[i] + areas[other_indices] - overlap_area)\n",
    "\n",
    "            # Find the overlapping predictions\n",
    "            overlapping_indices = other_indices[np.where(iou > self.IOU_THRESHOLD)[0]]\n",
    "            overlapping_indices = np.append(overlapping_indices, i)\n",
    "\n",
    "            # Set the probability of overlapping predictions to zero, and udpate max_probs and max_classes.\n",
    "            class_probs[overlapping_indices,max_classes[i]] = 0\n",
    "            max_probs[overlapping_indices] = np.amax(class_probs[overlapping_indices], axis=1)\n",
    "            max_classes[overlapping_indices] = np.argmax(class_probs[overlapping_indices], axis=1)\n",
    "\n",
    "        assert len(selected_boxes) == len(selected_classes) and len(selected_boxes) == len(selected_probs)\n",
    "        return selected_boxes, selected_classes, selected_probs\n",
    "\n",
    "    def _extract_bb(self, prediction_output, anchors):\n",
    "        assert len(prediction_output.shape) == 3\n",
    "        num_anchor = anchors.shape[0]\n",
    "        height, width, channels = prediction_output.shape\n",
    "        assert channels % num_anchor == 0\n",
    "\n",
    "        num_class = int(channels / num_anchor) - 5\n",
    "        assert num_class == len(self.labels)\n",
    "\n",
    "        outputs = prediction_output.reshape((height, width, num_anchor, -1))\n",
    "        \n",
    "        # Extract bouding box information\n",
    "        x = (self._logistic(outputs[...,0]) + np.arange(width)[np.newaxis, :, np.newaxis]) / width\n",
    "        y = (self._logistic(outputs[...,1]) + np.arange(height)[:, np.newaxis, np.newaxis]) / height\n",
    "        w = np.exp(outputs[...,2]) * anchors[:,0][np.newaxis, np.newaxis, :] / width\n",
    "        h = np.exp(outputs[...,3]) * anchors[:,1][np.newaxis, np.newaxis, :] / height\n",
    "\n",
    "        # (x,y) in the network outputs is the center of the bounding box. Convert them to top-left.\n",
    "        x = x - w / 2\n",
    "        y = y - h / 2\n",
    "        boxes = np.stack((x,y,w,h), axis=-1).reshape(-1, 4)\n",
    "\n",
    "        # Get confidence for the bounding boxes.\n",
    "        objectness = self._logistic(outputs[...,4])\n",
    "\n",
    "        # Get class probabilities for the bounding boxes.\n",
    "        class_probs = outputs[...,5:]\n",
    "        class_probs = np.exp(class_probs - np.amax(class_probs, axis=3)[..., np.newaxis])\n",
    "        class_probs = class_probs / np.sum(class_probs, axis=3)[..., np.newaxis] * objectness[..., np.newaxis]\n",
    "        class_probs = class_probs.reshape(-1, num_class)\n",
    "\n",
    "        assert len(boxes) == len(class_probs)\n",
    "        return (boxes, class_probs)\n",
    "    \n",
    "    def predict_image(self, image):\n",
    "        inputs = self.preprocess(image)\n",
    "        prediction_outputs = self.predict(inputs)\n",
    "        return self.postprocess(prediction_outputs)\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        image = image.convert(\"RGB\") if image.mode != \"RGB\" else image\n",
    "        image = image.resize((416, 416))\n",
    "        return image\n",
    "\n",
    "    def predict(self, preprocessed_inputs):\n",
    "        \"\"\"Evaluate the model and get the output\n",
    "\n",
    "        Need to be implemented for each platforms. i.e. TensorFlow, CoreML, etc.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def postprocess(self, prediction_outputs):\n",
    "        \"\"\" Extract bounding boxes from the model outputs.\n",
    "\n",
    "        Args:\n",
    "            prediction_outputs: Output from the object detection model. (H x W x C)\n",
    "\n",
    "        Returns:\n",
    "            List of Prediction objects.\n",
    "        \"\"\"\n",
    "        boxes, class_probs = self._extract_bb(prediction_outputs, self.ANCHORS)\n",
    "\n",
    "        # Remove bounding boxes whose confidence is lower than the threshold.\n",
    "        max_probs = np.amax(class_probs, axis=1)\n",
    "        index, = np.where(max_probs > self.prob_threshold)\n",
    "        index = index[(-max_probs[index]).argsort()]\n",
    "\n",
    "        # Remove overlapping bounding boxes\n",
    "        selected_boxes, selected_classes, selected_probs = self._non_maximum_suppression(boxes[index], class_probs[index], self.max_detections)\n",
    "\n",
    "        return [{'probability': round(float(selected_probs[i]), 8),\n",
    "                 'tagId': int(selected_classes[i]),\n",
    "                 'tagName': self.labels[selected_classes[i]],\n",
    "                 'boundingBox': {\n",
    "                     'left': round(float(selected_boxes[i][0]), 8),\n",
    "                     'top': round(float(selected_boxes[i][1]), 8),\n",
    "                     'width': round(float(selected_boxes[i][2]), 8),\n",
    "                     'height': round(float(selected_boxes[i][3]), 8)\n",
    "                 }\n",
    "             } for i in range(len(selected_boxes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested steps before inference: \n",
    "# 1. for an image of width and height being (w, h) pixels, resize image to (w', h'), where w/h = w'/h' and w' x h' = 262144\n",
    "# 2. resize network input size to (w', h')\n",
    "# 3. pass the image to network and do inference\n",
    "# (4. if inference speed is too slow for you, try to make w' x h' smaller)\n",
    "import sys\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "MODEL_FILENAME = 'model.pb'\n",
    "LABELS_FILENAME = 'labels.txt'\n",
    "\n",
    "class TFObjectDetection(ObjectDetection):\n",
    "    \"\"\"Object Detection class for TensorFlow\n",
    "    \"\"\"\n",
    "    def __init__(self, graph_def, labels):\n",
    "        super(TFObjectDetection, self).__init__(labels)\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            tf.import_graph_def(graph_def, name='')\n",
    "            \n",
    "    def predict(self, preprocessed_image):\n",
    "        inputs = np.array(preprocessed_image, dtype=np.float)[:,:,(2,1,0)] # RGB -> BGR\n",
    "\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            output_tensor = sess.graph.get_tensor_by_name('model_outputs:0')\n",
    "            outputs = sess.run(output_tensor, {'Placeholder:0': inputs[np.newaxis,...]})\n",
    "            return outputs[0]\n",
    "\n",
    "\n",
    "def main(image_filename):\n",
    "    # Load a TensorFlow model\n",
    "    graph_def = tf.GraphDef()\n",
    "    with tf.gfile.FastGFile(MODEL_FILENAME, 'rb') as f:\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # Load labels\n",
    "    with open(LABELS_FILENAME, 'r') as f:\n",
    "        labels = [l.strip() for l in f.readlines()]\n",
    "\n",
    "    od_model = TFObjectDetection(graph_def, labels)\n",
    "\n",
    "    image = Image.open(image_filename)\n",
    "    predictions = od_model.predict_image(image)\n",
    "    total=len(predictions)\n",
    "    if(total>0):\n",
    "        objects[\"text\"]=str(total)+\" Object(s) detected...\"\n",
    "    else:\n",
    "        objects[\"text\"]=\"No objects detected\"\n",
    "    for i in predictions:\n",
    "        filename=os.path.splitext(image_filename)[0]\n",
    "        extract_name_plate_bbox(image_filename,i[\"boundingBox\"][\"left\"],i[\"boundingBox\"][\"top\"],i[\"boundingBox\"][\"width\"],i[\"boundingBox\"][\"height\"],filename+\"_plate.jpg\",i[\"probability\"])\n",
    "def predict(filename):\n",
    "    main(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import *\n",
    "#OCR Utility Function.\n",
    "def perform_OCR(plate,name):\n",
    "    cv2.imwrite(name,plate)\n",
    "    text=azureOCR(name)\n",
    "    result[\"text\"]=text\n",
    "    window.mainloop()\n",
    "    \n",
    "#extract nameplate from the image.\n",
    "def extract_name_plate_bbox(path,left,top,width,height,name,probability):      \n",
    "    img=cv2.imread(path)\n",
    "    x = int(left * img.shape[1])\n",
    "    y = int(top * img.shape[0])\n",
    "    x2 = x + int(width * img.shape[1])\n",
    "    y2 = y + int(height * img.shape[0])\n",
    "    #crop image at name plate\n",
    "    plate=img[y:y2,x:x2]\n",
    "    #resize the image.\n",
    "    plate=cv2.resize(plate,(0,0),fx=2,fy=2)\n",
    "    cv2.imwrite(path,img)\n",
    "    img = cv2.rectangle(img, (x,y), (x2,y2), (0,0,255), 2)\n",
    "    background = cv2.cvtColor(plate, cv2.COLOR_BGR2RGB)\n",
    "    photo = ImageTk.PhotoImage(image = PIL.Image.fromarray(background)) \n",
    "    canvas = Canvas(window, width = plate.shape[1], height = plate.shape[0])\n",
    "    canvas.create_image(0, 0, image=photo, anchor=tk.NW)\n",
    "    canvas.pack(side=LEFT,padx=30)\n",
    "    perform_OCR(plate,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "if 'COGNITIVE_SERVICE_KEY' in os.environ:\n",
    "    subscription_key = os.environ['COGNITIVE_SERVICE_KEY']\n",
    "else:\n",
    "    print(\"Set the COMPUTER_VISION_SUBSCRIPTION_KEY environment variable.\")\n",
    "    sys.exit()\n",
    "\n",
    "if 'COMPUTER_VISION_ENDPOINT' in os.environ:\n",
    "    endpoint = os.environ['COMPUTER_VISION_ENDPOINT']\n",
    "\n",
    "ocr_url = endpoint + \"vision/v2.0/ocr\"\n",
    "params = {'language': 'unk', 'detectOrientation': 'true'}\n",
    "def azureOCR(filename):\n",
    "    extract[\"text\"]=\"extracting text\"\n",
    "    image_data = open(filename, \"rb\").read()\n",
    "    headers = {'Ocp-Apim-Subscription-Key': subscription_key, 'Content-Type': 'application/octet-stream'}\n",
    "    response = requests.post(ocr_url, headers=headers, params=params, data = image_data)\n",
    "    analysis = response.json()\n",
    "    line_infos = [region[\"lines\"] for region in analysis[\"regions\"]]\n",
    "    word_infos = []\n",
    "    for line in line_infos:\n",
    "        for word_metadata in line:\n",
    "            for word_info in word_metadata[\"words\"]:\n",
    "                word_infos.append(word_info)\n",
    "    word_infos\n",
    "    text=\"\"\n",
    "    for word in word_infos:\n",
    "        bbox = [int(num) for num in word[\"boundingBox\"].split(\",\")]\n",
    "        text += word[\"text\"]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectfile():\n",
    "    filename = askopenfilename(title = \"Select Image to perform LPR\",filetypes = [(\"All pictures\",\"*.jpg *.png\")])\n",
    "     # show an \"Open\" dialog box and return the path to the selected file\n",
    "    background = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n",
    "    cv2.resize(background,(0,0),fx=2,fy=2)\n",
    "    photo = ImageTk.PhotoImage(image = PIL.Image.fromarray(background)) \n",
    "    canvas[\"width\"]=background.shape[1]\n",
    "    canvas[\"height\"]=background.shape[0]\n",
    "    btn_blur.destroy()\n",
    "    canvas.create_image(0, 0, image=photo, anchor=tk.NW)\n",
    "    predict(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}